{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(pred_path):\n",
    "    \"\"\"\n",
    "    Load all predictions into one dictionary with structure \n",
    "    {\"model_name\":{\"question_id\": \"prediction\"}}\n",
    "    + The list of models are predefined below\n",
    "\n",
    "    INPUT:\n",
    "        pred_path: all predictions of models are saved in the same folder\n",
    "    OUTPUT:\n",
    "        pred_dictionary: all predictions of the six adversarial models \n",
    "                            {\"model_name\":{\"question_id\": \"prediction\"}}\n",
    "    \"\"\"\n",
    "    models = [\"bert-base-cased\", \"roberta-base\", \"SpanBERT/spanbert-base-cased\",\n",
    "              \"bert-large-cased\", \"roberta-large\", \"SpanBERT/spanbert-large-cased\"]\n",
    "    chunks = [\"\"]\n",
    "    pred_dictionary = {}\n",
    "    for model in models:\n",
    "        pred_model = {}\n",
    "        for chunk in chunks:\n",
    "            complete_path = os.path.join(\n",
    "                pred_path, model, chunk, \"nbest_predictions_eval.json\")\n",
    "            with open(complete_path) as dataset_file:\n",
    "                pred_model.update(json.load(dataset_file))\n",
    "        shorten_pred_model = {}\n",
    "        for id in pred_model:\n",
    "            shorten_pred_model[id] = pred_model[id][0]\n",
    "        pred_dictionary[model] = shorten_pred_model\n",
    "    return pred_dictionary\n",
    "\n",
    "def extract_hard_unans(tfidf, all_predictions):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    hard_unans = {'1': [], '2': [], '3': [], '4': [], '5': [], '6': []}\n",
    "    difficulty_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0}\n",
    "    for qas in tfidf:\n",
    "        id = qas['id']\n",
    "        qas_pred = []                           # [[prediction, confidence]]\n",
    "        num_models = len(all_predictions)       # number of adversarial models\n",
    "        for model in all_predictions:\n",
    "            model_pred = all_predictions[model][id]['text']\n",
    "            if model_pred != \"\":\n",
    "                qas_pred.append(model_pred)\n",
    "\n",
    "        difficulty_level = len(qas_pred)\n",
    "        difficulty_dict[difficulty_level] += 1\n",
    "        if difficulty_level == 0:\n",
    "            continue\n",
    "        elif difficulty_level == 1:\n",
    "            hard_unans['1'].append(qas)\n",
    "        elif difficulty_level == 2:\n",
    "            hard_unans['2'].append(qas)\n",
    "        elif difficulty_level == 3:\n",
    "            hard_unans['3'].append(qas)\n",
    "        elif difficulty_level == 4:\n",
    "            hard_unans['4'].append(qas)\n",
    "        elif difficulty_level == 5:\n",
    "            hard_unans['5'].append(qas)\n",
    "        elif difficulty_level == 6:\n",
    "            hard_unans['6'].append(qas)\n",
    "    print(difficulty_dict)\n",
    "    return hard_unans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_path = \"/Volumes/Share/tran_s2/squad_devXtrain/Data/raw_tfidf/raw_tfidf_of_squad1_devXtrain/raw_tfidf_of_squad1_devXtrain-new-format.json\"\n",
    "with open(tfidf_path) as dataset_file:\n",
    "    tfidf = json.load(dataset_file)['data']\n",
    "\n",
    "pred_path = \"/Volumes/Share/tran_s2/squad_devXtrain/Result/squad_tfidf_model/eval_squad1_devXtrain\"\n",
    "predictions_dictionary = load_predictions(pred_path)\n",
    "hard_unans = extract_hard_unans(tfidf, predictions_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/Volumes/Share/tran_s2/squad_devXtrain/Data/squad_tfidf_hard_unans/verMay21\"\n",
    "for type in hard_unans:\n",
    "    type_file = type + \".json\"\n",
    "    save_path = os.path.join(folder_path, type_file)\n",
    "    to_save = {'data': hard_unans[type]}\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(to_save, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
