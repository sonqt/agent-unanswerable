{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "This notebook is used for finalizing the AGent dataset.\n",
    "Firstly, we need to manually annotate 200 challenging unanswerable candidates. Some challenging unanswerable candidates are actually answerable. Save the ids of these answerable questions into `answerable_ids_path`.\n",
    "\n",
    "Get the predictions of adversarial models on 200 annotated challenging unanswerable candidates. Save the predictions in `annotated_pred_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_pred_path = \"Prediction/Step3_Annotated\"\n",
    "answerable_ids_path = \"answerable_ids.json\"\n",
    "\n",
    "full_pred_dev_path = \"src/step1/retriever_component/unans_cdd/squad_dev_unans_cdd.json\"      # path to predictions of the 6 adversarial models\n",
    "dev_path = \"Prediction/Adversarial_Models/SQuAD_Dev\"    # path to all challenging unanswerable candidates (product of step 2)\n",
    "squadv1_dev_path = \"src/step1/data/SQuAD/original/squad_dev.json\"\n",
    "save_dev_path = \"AGent_data/SQuAD/dev.json\"\n",
    "\n",
    "\n",
    "tfidf_train_path = \"src/step1/retriever_component/unans_cdd/squad_train_unans_cdd.json\"\n",
    "full_pred_train_path = \"Prediction/Adversarial_Models/SQuAD_Train\"\n",
    "squadv1_train_path = \"src/step1/data/SQuAD/original/squad_train.json\"\n",
    "save_train_path = \"AGent_data/SQuAD/train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(pred_path):\n",
    "    \"\"\"\n",
    "    Load all predictions into one dictionary with structure \n",
    "    {\"model_name\":{\"question_id\": \"prediction\"}}\n",
    "    + The list of models are predefined below\n",
    "\n",
    "    INPUT:\n",
    "        pred_path: all predictions of models are saved in the same folder\n",
    "    OUTPUT:\n",
    "        pred_dictionary:\n",
    "    \"\"\"\n",
    "    models = [\"bert-base-cased\", \"roberta-base\", \"SpanBERT/spanbert-base-cased\",\n",
    "              \"bert-large-cased\", \"roberta-large\", \"SpanBERT/spanbert-large-cased\"]\n",
    "    pred_dictionary = {}\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        pred_model = {}\n",
    "        complete_path = os.path.join(\n",
    "            pred_path, model, \"nbest_predictions_eval.json\")\n",
    "        with open(complete_path) as dataset_file:\n",
    "            pred_model = json.load(dataset_file)\n",
    "        if i == 0:\n",
    "            for id in pred_model:\n",
    "                pred_dictionary[id] = [pred_model[id][0]]\n",
    "        else:\n",
    "            for id in pred_model:\n",
    "                pred_dictionary[id].append(pred_model[id][0])\n",
    "    return pred_dictionary\n",
    "\n",
    "def load_answerable_ids(id_path):\n",
    "    with open(id_path) as dataset_file:\n",
    "        data = json.load(dataset_file)\n",
    "    answerable_ids = data['ids']\n",
    "    return answerable_ids\n",
    "\n",
    "def tune_formula(all_predictions, answerable_ids, tuning_range, formula):\n",
    "    \"\"\"\n",
    "    Use the grid search to find best parameters in the tuning_range\n",
    "\n",
    "    INPUT:\n",
    "        all_predictions: predictions on all ids by all models\n",
    "        tuning_range: (list of list) in the format of [[start, end, step],[start, end, step]]\n",
    "        formula: the formula that we are tuning\n",
    "    OUTPUT:\n",
    "        best_recall:\n",
    "        best_threshold:\n",
    "        best parameters:\n",
    "    \"\"\"\n",
    "    best_recall, best_threshold = 0, 0\n",
    "    best_parameters = [-1 for _ in range(len(tuning_range))]\n",
    "    all_parameters = [list(np.arange(tune[0], tune[1], tune[2])) for tune in tuning_range]\n",
    "    for parameters in product(*all_parameters):\n",
    "        current_threshold, current_recall = calculate_best_recall(all_predictions, answerable_ids,\n",
    "                                                                  parameters, formula)\n",
    "        if current_recall > best_recall:\n",
    "            best_recall = current_recall\n",
    "            best_threshold = current_threshold\n",
    "            best_parameters = parameters\n",
    "    return best_recall, best_threshold, best_parameters\n",
    "\n",
    "def calculate_best_recall(all_predictions, answerable_ids, parameters, formula):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        all_predictions: predictions on all ids by all models\n",
    "        parameters: set of parameters that we are considering\n",
    "        formula: the formula that we are tuning\n",
    "    \"\"\"\n",
    "    answerable_values = []\n",
    "    unanswerable_values = []\n",
    "    for id in all_predictions:\n",
    "        value = formula(all_predictions[id], parameters)\n",
    "        if id in answerable_ids:\n",
    "            answerable_values.append(value)\n",
    "        else:\n",
    "            unanswerable_values.append(value)\n",
    "    threshold = min(answerable_values)\n",
    "    recall = len(list(filter(lambda x: (x < threshold), unanswerable_values))) / len(unanswerable_values)\n",
    "    return threshold, recall\n",
    "\n",
    "def formula(predictions, parameters):\n",
    "    \"\"\"\n",
    "    This is the formular descibed in the appendix of the paper.\n",
    "\n",
    "    INPUT:\n",
    "        predictions: (list of dict) list of 6 predictions on one id\n",
    "        parameters: (list of float) list of all parameters needed\n",
    "    OUTPUT:\n",
    "        value: The value V(q) of the corresponding question\n",
    "    \"\"\"\n",
    "    assert len(parameters) == 2\n",
    "    [alpha, beta] = parameters\n",
    "    unans_confidence, unans_count = 0, 0\n",
    "    ans_confidence, ans_count = 0, 0\n",
    "    for pred in predictions:\n",
    "        if pred['text'] != \"\":\n",
    "            ans_confidence += pred['probability']\n",
    "            ans_count += 1\n",
    "        else:\n",
    "            unans_confidence += pred['probability']\n",
    "            unans_count += 1\n",
    "    ans_average = ans_confidence / ans_count if ans_count > 0 else ans_confidence\n",
    "    unans_average = unans_confidence / unans_confidence if unans_count > 0 else unans_confidence\n",
    "    value =  ans_confidence * (alpha**ans_count) - unans_confidence * (beta**unans_count)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "38\n",
      "We can achieve recall 0.42592592592592593 with parameters (0.64, 0.6900000000000001) and threshold -0.3428283339143994\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We manually annotated 200 questions for tuning the formula. \n",
    "Get the predictions of the 6 adversarial models on these 200 questions.\n",
    "Use these predictions to tune the formula and then apply the formula on the rest of the data.\n",
    "\"\"\"\n",
    "annotated_pred_path = \"/Volumes/Share/tran_s2/squad_devXtrain/Result/squad_tfidf/annotated_question\"\n",
    "all_predictions = load_predictions(pred_path)\n",
    "print(len(all_predictions))\n",
    "answerable_ids_path = \"/Volumes/Share/tran_s2/squad_devXtrain/Data/answerable_id.json\"\n",
    "answerable_ids = load_answerable_ids(answerable_ids_path)\n",
    "print(len(answerable_ids))\n",
    "best_recall, best_threshold, best_parameters = tune_formula(all_predictions, answerable_ids, [[0,2,0.01], [0,2,0.01]], formula_2)\n",
    "print(\"We can achieve recall {} with parameters {} and threshold {}\".format(best_recall, best_parameters, best_threshold))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Using Tuned Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difficulty(predictions):\n",
    "    difficulty_level = 0\n",
    "    for pred in predictions:\n",
    "        if pred['text'] != \"\":\n",
    "            difficulty_level += 1\n",
    "    return difficulty_level\n",
    "def extract_unans(data_path, full_pred_path, formula, parameters, threshold):\n",
    "    \"\"\"\n",
    "    Extract the product of AGent pipeline given the tuned formula.\n",
    "    Condition: (V(q) < threshold) && (difficulty(q) >= 2)\n",
    "    INPUT:\n",
    "        data_path: path to the full data of challenging unanswerable candidates\n",
    "        full_pred_path: path to predictions of 6 adversarial models on challenging unanswerable candidates\n",
    "        formula: tuned formula\n",
    "        parameters: parameters for the formula\n",
    "        threshold: threshold of the unanswerable questions \n",
    "    OUTPUT:\n",
    "        data_save: the unanswerable questions created by AGent.\n",
    "    \"\"\"\n",
    "    with open(data_path) as dataset_file:\n",
    "        data = json.load(dataset_file)['data']\n",
    "    data_save = []\n",
    "    \n",
    "    all_predictions = load_predictions(full_pred_path)\n",
    "    print(len(all_predictions))\n",
    "    survey_dict = {2:0, 3:0, 4:0, 5:0, 6:0}\n",
    "\n",
    "    for qas in data:\n",
    "        id = qas['id']\n",
    "        value = formula(all_predictions[id], parameters)\n",
    "        if value < threshold and difficulty(all_predictions[id]) >= 2:\n",
    "            data_save.append(qas)\n",
    "            survey_dict[difficulty(all_predictions[id])] +=1\n",
    "    print(survey_dict)\n",
    "    return data_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7117\n",
      "{2: 853, 3: 743, 4: 603, 5: 189, 6: 0}\n",
      "2388\n"
     ]
    }
   ],
   "source": [
    "# Apply the formula on the dev set.\n",
    "full_pred_dev_path = \"/Volumes/Share/tran_s2/squad_devXtrain/Result/squad_tfidf/for_extract_unans\"      # path to predictions of the 6 adversarial models\n",
    "dev_path = \"/Volumes/Share/tran_s2/squad_devXtrain/Data/squad_tfidf_hard_unans/verMay23/2.json\"    # path to all challenging unanswerable candidates (product of step 2)\n",
    "dev_unans = extract_unans(dev_path, full_pred_dev_path, formula_2, best_parameters, best_threshold)\n",
    "print(len(dev_unans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dev\n",
    "squadv1_dev_path = \"/Volumes/Share/tran_s2/Public_Datasets/mrqa/mrqa_dev/squad1/SQuAD-train-from-MRQA-new-format.json\"\n",
    "with open(squadv1_dev_path) as dataset_file:\n",
    "    dataset = json.load(dataset_file)['data']\n",
    "dataset.extend(dev_unans)\n",
    "random.shuffle(dataset)\n",
    "to_save = {\"version\": \"SQuAD AGent dev\", \"data\": dataset}\n",
    "json_object = json.dumps(to_save, indent=4)\n",
    "save_dev_path = \"/Volumes/Share/tran_s2/squad_devXtrain/Data/squad_tfidf_hard_unans/SQuAD_train/SQuAD_AGent_train.json\"\n",
    "with open(save_dev_path, \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870588\n",
      "{2: 31951, 3: 9720, 4: 4809, 5: 1536, 6: 0}\n",
      "48016\n"
     ]
    }
   ],
   "source": [
    "# Apply the formula on the train set.\n",
    "tfidf_train_path = \"/Volumes/Share/tran_s2/squad_devXtrain/Data/raw_tfidf/raw_tfidf_of_squad1_train_top10/raw_tfidf_of_squad1_train_top10-new-format.json\"\n",
    "full_pred_train_path = \"/Volumes/Share/tran_s2/squad_devXtrain/Result/squad_tfidf_model/eval_squad1_train_top10\"\n",
    "train_unans = extract_unans(tfidf_train_path, full_pred_train_path, formula_2, best_parameters, best_threshold)\n",
    "print(len(train_unans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train\n",
    "with open(squadv1_train_path) as dataset_file:\n",
    "    dataset = json.load(dataset_file)['data']\n",
    "dataset.extend(train_unans)\n",
    "random.shuffle(dataset)\n",
    "to_save = {\"version\": \"SQuAD AGent\", \"data\": dataset}\n",
    "json_object = json.dumps(to_save, indent=4)\n",
    "with open(save_train_path, \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
